{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Import data handling libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import utilities for displaying and progress tracking\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "# Import Transformers library components\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificare la disponibilità di cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    30000\n",
       "3    30000\n",
       "1    30000\n",
       "0    30000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/wangrongsheng/ag_news/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/wangrongsheng/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "# World (0), Sports (1), Business (2), Sci/Tech (3).\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il file JSON come JSON Lines\n",
    "df = pd.read_json('dataset.json', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dswal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Scarica le stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# Definizione della funzione clean_text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, '')\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text)\n",
    "    emoji_pattern = re.compile(\"[\"  \n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL primo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "2    31900\n",
      "3    31900\n",
      "1    31900\n",
      "0    31900\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    4400\n",
      "1    4400\n",
      "2    4400\n",
      "3    4400\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dswal\\AppData\\Local\\Temp\\ipykernel_8320\\2651934709.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_combined.groupby('label').apply(lambda x: x.sample(n=4400, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Uniamo i due dataset\n",
    "df_combined = pd.concat([df, df_test])\n",
    "\n",
    "# Contiamo le occorrenze di ciascuna classe nel dataset combinato\n",
    "class_counts = df_combined['label'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Per ogni classe, prendiamo solo 4400 elementi\n",
    "df_final = df_combined.groupby('label').apply(lambda x: x.sample(n=4400, random_state=42))\n",
    "\n",
    "# Rimuoviamo l'indice multi-livello creato dal groupby\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "# Controlliamo che abbiamo 4400 elementi per classe\n",
    "print(df_final['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (14080, 2)\n",
      "Test set size: (3520, 2)\n"
     ]
    }
   ],
   "source": [
    "# Eseguiamo lo split 80% - 20%\n",
    "df_train, df_test_split = train_test_split(df_final, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verifichiamo le dimensioni dei dataset risultanti\n",
    "print(f\"Training set size: {df_train.shape}\")\n",
    "print(f\"Test set size: {df_test_split.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL secondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['category', 'short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209527"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['short_description'].str.split().str.len() >= 20) & (df['short_description'].str.split().str.len() <= 100) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "WELLNESS          15568\n",
       "POLITICS           9608\n",
       "PARENTING          7605\n",
       "TRAVEL             7401\n",
       "STYLE & BEAUTY     7117\n",
       "ENTERTAINMENT      3939\n",
       "BUSINESS           3373\n",
       "WEDDINGS           3039\n",
       "DIVORCE            2861\n",
       "HEALTHY LIVING     2844\n",
       "QUEER VOICES       2705\n",
       "FOOD & DRINK       2668\n",
       "HOME & LIVING      2403\n",
       "IMPACT             2243\n",
       "PARENTS            1752\n",
       "BLACK VOICES       1714\n",
       "WOMEN              1555\n",
       "MONEY              1463\n",
       "COMEDY             1406\n",
       "SPORTS             1319\n",
       "WORLDPOST          1166\n",
       "GREEN              1141\n",
       "ENVIRONMENT        1066\n",
       "RELIGION           1060\n",
       "FIFTY              1021\n",
       "WORLD NEWS         1016\n",
       "CRIME               989\n",
       "THE WORLDPOST       879\n",
       "TECH                877\n",
       "SCIENCE             869\n",
       "ARTS                823\n",
       "CULTURE & ARTS      816\n",
       "MEDIA               746\n",
       "EDUCATION           712\n",
       "U.S. NEWS           695\n",
       "COLLEGE             563\n",
       "TASTE               545\n",
       "STYLE               338\n",
       "LATINO VOICES       337\n",
       "GOOD NEWS           254\n",
       "WEIRD NEWS          250\n",
       "ARTS & CULTURE      184\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_include = ['WELLNESS', 'POLITICS', 'TRAVEL', 'ENTERTAINMENT', 'BUSINESS']\n",
    "df = df[df['category'].isin(categories_to_include)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dswal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Applicazione della funzione alla colonna 'short_description'\n",
    "df['short_description_cleaned'] = df['short_description'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37550"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilanciamento secondo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "BUSINESS         3183\n",
       "ENTERTAINMENT    3183\n",
       "POLITICS         3183\n",
       "TRAVEL           3183\n",
       "WELLNESS         3183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_class_size = df['category'].value_counts().min()\n",
    "\n",
    "# Seleziona 3000 elementi per ogni categoria mantenendo la distribuzione dei token\n",
    "sampled_dfs = []\n",
    "for category, group in df.groupby('category'):\n",
    "    sampled_group = group.sample(n=min(minority_class_size, len(group)), random_state=42)\n",
    "    sampled_dfs.append(sampled_group)\n",
    "\n",
    "# Combina i campioni in un unico DataFrame\n",
    "balanced_df = pd.concat(sampled_dfs)\n",
    "\n",
    "# Controlla il risultato\n",
    "balanced_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "WELLNESS         14463\n",
       "POLITICS          9147\n",
       "TRAVEL            6981\n",
       "ENTERTAINMENT     3776\n",
       "BUSINESS          3183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un'istanza di LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Trasforma la colonna 'category' in numerica\n",
    "balanced_df['category'] = label_encoder.fit_transform(balanced_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrispondenza tra categorie e numeri:\n",
      "{'BUSINESS': 0, 'ENTERTAINMENT': 1, 'POLITICS': 2, 'TRAVEL': 3, 'WELLNESS': 4}\n"
     ]
    }
   ],
   "source": [
    "# Visualizza la corrispondenza tra le categorie e i numeri\n",
    "category_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "\n",
    "print(\"Corrispondenza tra categorie e numeri:\")\n",
    "print(category_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione nel training set:\n",
      "category\n",
      "1    2547\n",
      "4    2547\n",
      "3    2546\n",
      "2    2546\n",
      "0    2546\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuzione nel testing set:\n",
      "category\n",
      "2    637\n",
      "0    637\n",
      "3    637\n",
      "1    636\n",
      "4    636\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dividi i dati bilanciati in 80-20\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['category'], random_state=42)\n",
    "\n",
    "# Controlla le distribuzioni\n",
    "print(\"Distribuzione nel training set:\")\n",
    "print(train_df['category'].value_counts())\n",
    "\n",
    "print(\"\\nDistribuzione nel testing set:\")\n",
    "print(test_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(train_df['short_description'])\n",
    "labels = list(train_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  With Easter just around the corner, kid-friendly resortsare offering spring savings along with bunny-themed events and thousands upon thousands of hidden eggs.\n",
      "╒═══════════╤═════════════╕\n",
      "│ Tokens    │   Token IDs │\n",
      "╞═══════════╪═════════════╡\n",
      "│ with      │        2007 │\n",
      "├───────────┼─────────────┤\n",
      "│ easter    │       10957 │\n",
      "├───────────┼─────────────┤\n",
      "│ just      │        2074 │\n",
      "├───────────┼─────────────┤\n",
      "│ around    │        2105 │\n",
      "├───────────┼─────────────┤\n",
      "│ the       │        1996 │\n",
      "├───────────┼─────────────┤\n",
      "│ corner    │        3420 │\n",
      "├───────────┼─────────────┤\n",
      "│ ,         │        1010 │\n",
      "├───────────┼─────────────┤\n",
      "│ kid       │        4845 │\n",
      "├───────────┼─────────────┤\n",
      "│ -         │        1011 │\n",
      "├───────────┼─────────────┤\n",
      "│ friendly  │        5379 │\n",
      "├───────────┼─────────────┤\n",
      "│ resorts   │       16511 │\n",
      "├───────────┼─────────────┤\n",
      "│ ##are     │       12069 │\n",
      "├───────────┼─────────────┤\n",
      "│ offering  │        5378 │\n",
      "├───────────┼─────────────┤\n",
      "│ spring    │        3500 │\n",
      "├───────────┼─────────────┤\n",
      "│ savings   │       10995 │\n",
      "├───────────┼─────────────┤\n",
      "│ along     │        2247 │\n",
      "├───────────┼─────────────┤\n",
      "│ with      │        2007 │\n",
      "├───────────┼─────────────┤\n",
      "│ bunny     │       16291 │\n",
      "├───────────┼─────────────┤\n",
      "│ -         │        1011 │\n",
      "├───────────┼─────────────┤\n",
      "│ themed    │       11773 │\n",
      "├───────────┼─────────────┤\n",
      "│ events    │        2824 │\n",
      "├───────────┼─────────────┤\n",
      "│ and       │        1998 │\n",
      "├───────────┼─────────────┤\n",
      "│ thousands │        5190 │\n",
      "├───────────┼─────────────┤\n",
      "│ upon      │        2588 │\n",
      "├───────────┼─────────────┤\n",
      "│ thousands │        5190 │\n",
      "├───────────┼─────────────┤\n",
      "│ of        │        1997 │\n",
      "├───────────┼─────────────┤\n",
      "│ hidden    │        5023 │\n",
      "├───────────┼─────────────┤\n",
      "│ eggs      │        6763 │\n",
      "├───────────┼─────────────┤\n",
      "│ .         │        1012 │\n",
      "╘═══════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "print(' Original: ', sentences[index])\n",
    "\n",
    "table = np.array([tokenizer.tokenize(sentences[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[index]))]).T\n",
    "print(tabulate(table,headers = ['Tokens', 'Token IDs'],tablefmt = 'fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  84\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sentence in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dswal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every tweet...\n",
    "for sentence in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  One of the key components of transformation and innovation is the business model, and since the ability of companies to transform and reinvent themselves is crucial to their lifeline, I went straight to the source.\n",
      "Input IDs: tensor([  101,  2028,  1997,  1996,  3145,  6177,  1997,  8651,  1998,  8144,\n",
      "         2003,  1996,  2449,  2944,  1010,  1998,  2144,  1996,  3754,  1997,\n",
      "         3316,  2000, 10938,  1998, 27788, 15338,  3209,  2003, 10232,  2000,\n",
      "         2037,  2166,  4179,  1010,  1045,  2253,  3442,  2000,  1996,  3120,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0, now as a list of IDs.\n",
    "i=random.randint(0,len(sentences)-1)\n",
    "print('Original: ', sentences[i])\n",
    "print('Input IDs:', input_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Kathy discovered that her son, Erik, was snorting heroin, she decided to give it a try, too. “I’m kind of embarrassed\n",
      "╒═════════════╤═════════════╤══════════════════╕\n",
      "│ Tokens      │   Token IDs │   Attention Mask │\n",
      "╞═════════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]       │         101 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ when        │        2043 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ kathy       │       14986 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ discovered  │        3603 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ that        │        2008 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ her         │        2014 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ son         │        2365 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ,           │        1010 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ erik        │       10240 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ,           │        1010 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ was         │        2001 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ snort       │       26759 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ##ing       │        2075 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ heroin      │       19690 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ,           │        1010 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ she         │        2016 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ decided     │        2787 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ to          │        2000 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ give        │        2507 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ it          │        2009 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ a           │        1037 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ try         │        3046 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ,           │        1010 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ too         │        2205 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ .           │        1012 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ “           │        1523 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ i           │        1045 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ ’           │        1521 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ m           │        1049 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ kind        │        2785 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ of          │        1997 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ embarrassed │       10339 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]       │         102 │                1 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "├─────────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]       │           0 │                0 │\n",
      "╘═════════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(sentences) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(input_ids[index]))\n",
    "  token_ids = [i.numpy() for i in input_ids[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(sentences[index])\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,185 training samples\n",
      "2,547 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "#val_size = int(0.2 * len(dataset))\n",
    "val_size = len(dataset)  - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per eseguire il fine-tuning di BERT su un task specifico, è raccomandata una dimensione del batch pari a 16 o a 32\n",
    "batch_size = 16\n",
    "\n",
    "#Crea il DataLoaders per il training set e per il validation set\n",
    "#Vengono presi gli elementoi del training set in ordine randomico\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  #Il training set\n",
    "            sampler = RandomSampler(train_dataset), #Seleziona randomicamente i batch\n",
    "            batch_size = batch_size #Esegue l'allenamento con questa dimensione dei batch\n",
    "        )\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, #Il validation set\n",
    "            sampler = SequentialSampler(val_dataset), #Seleziona i batch in maniera sequenziale\n",
    "            batch_size = batch_size #Esegue una valutazione del modello con questa dimensione dei batch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione modello BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# if device == \"cuda:0\":\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "#     model = model.cuda()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_tweets=['WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.']\n",
    "for tweet in test_tweets:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = max_len,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predictions.extend(list(pred_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['tweets']=test_tweets\n",
    "df_output['label'] =predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
